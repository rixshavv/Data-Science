{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nlp\n",
    "# step 1: text-preprocessing-Tokenization,Lemmatization,Stemming,stopwords\n",
    "\n",
    "#Step 2: converting text data into vectors(bow,unigram,Bigram)\n",
    "\n",
    "#Step 3; word to vec,avg wordto vec(deeplearnig technique)\n",
    "#Step 4; Neural networks like rnn,lstm,rnn\n",
    "\n",
    "#transformer\n",
    "\n",
    "#bert\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:51:47.107745Z",
     "start_time": "2025-06-04T09:51:47.098125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#corpus-Paragraph, Documents-Sentences, Vocabulary-Unique words\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "word=\"I like to eat apple's. I am Happy! I am in control.\"\n",
    "sent_tokenize(word)\n"
   ],
   "id": "ec67ccd79c141064",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I like to eat apple's.\", 'I am Happy!', 'I am in control.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:48:12.511112Z",
     "start_time": "2025-06-04T09:48:12.503901Z"
    }
   },
   "cell_type": "code",
   "source": "document=sent_tokenize(word)",
   "id": "bdc3946caf4a2c26",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:46:11.489727Z",
     "start_time": "2025-06-04T09:46:11.482318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in document:\n",
    "    print(word_tokenize(i)) #convert the sentence into words"
   ],
   "id": "eeedfd2bd5e04693",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'to', 'eat', 'apple', '.']\n",
      "['I', 'am', 'Happy', '!']\n",
      "['I', 'am', 'in', 'control']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:48:18.793363Z",
     "start_time": "2025-06-04T09:48:18.782075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Paragraph -into words & sentence into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(word) #converts the paragraph into words"
   ],
   "id": "889a3cf28e87a5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'apple',\n",
       " \"'s\",\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Happy',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'in',\n",
       " 'control']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:47:19.150853Z",
     "start_time": "2025-06-04T09:47:19.139200Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.tokenize import wordpunct_tokenize",
   "id": "54dbaf7e0df65cdc",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:48:32.463862Z",
     "start_time": "2025-06-04T09:48:32.455053Z"
    }
   },
   "cell_type": "code",
   "source": "wordpunct_tokenize(word) #this also includes the punctuation in the tokenization process",
   "id": "f39df607a43b8d1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'apple',\n",
       " \"'\",\n",
       " 's',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Happy',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'in',\n",
       " 'control']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:51:51.146098Z",
     "start_time": "2025-06-04T09:51:51.137048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(word) #full stop last wlaa ko seperate consider karega rest are included in prev words."
   ],
   "id": "6b79ad923109acd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'eat',\n",
       " \"apple's.\",\n",
       " 'I',\n",
       " 'am',\n",
       " 'Happy',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'in',\n",
       " 'control',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
